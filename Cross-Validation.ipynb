{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a54305-1ce4-4dd5-842a-ae3f586f1b58",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #00008B; padding: 20px;\">\n",
    "    <h1 style=\"font-size: 100px; color: #ffffff;\">Cross-Validation</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d71dac-9bee-4f6d-852f-e97927041d04",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "  .highlight {\n",
    "    background-color: #e8f5e9;\n",
    "    padding: 15px;\n",
    "    border-radius: 10px;\n",
    "    border: 1px solid #a5d6a7;\n",
    "    color: #2e7d32;\n",
    "  }\n",
    "  .highlight h3 {\n",
    "    color: #388e3c;\n",
    "  }\n",
    "  .highlight p, .highlight li {\n",
    "    color: #1b5e20;\n",
    "  }\n",
    "</style>\n",
    "\n",
    "<div class=\"highlight\">\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "**Cross-Validation** is a technique used in data mining and machine learning to assess the performance and generalizability of a model. It involves partitioning the dataset into subsets, training the model on some subsets (training set) and evaluating it on the remaining subsets (validation set). This process helps in ensuring that the model's performance is consistent and not overly dependent on the particular sample of data used.\n",
    "\n",
    "**Types of Cross-Validation:**\n",
    "\n",
    "1. **K-Fold Cross-Validation**: The dataset is divided into $K$ subsets (folds). The model is trained on $K-1$ folds and tested on the remaining fold. This process is repeated $K$ times, with each fold serving as the validation set once. The performance metric is averaged over the $K$ iterations.\n",
    "   \n",
    "2. **Leave-One-Out Cross-Validation (LOOCV)**: A special case of K-fold cross-validation where $K$ is equal to the number of data points. Each data point is used once as the validation set, and the model is trained on the remaining data points.\n",
    "   \n",
    "3. **Stratified K-Fold Cross-Validation**: Similar to K-fold cross-validation but ensures that each fold has approximately the same percentage of samples of each target class.\n",
    "\n",
    "### 1-SE Rule with Cross-Validation\n",
    "\n",
    "The **1-SE Rule** (1 Standard Error Rule) is a heuristic used to select a simpler model from a set of candidate models that all perform similarly. The rule aims to balance model complexity and performance by choosing the simplest model whose performance is within one standard error of the best performing model.\n",
    "\n",
    "**Steps to Apply the 1-SE Rule:**\n",
    "\n",
    "1. **Perform Cross-Validation**: Conduct cross-validation (e.g., K-fold) to evaluate the performance of different models and calculate the performance metric (e.g., mean squared error, accuracy) along with its standard error.\n",
    "\n",
    "2. **Identify the Best Model**: Determine the model with the best performance metric (e.g., lowest error).\n",
    "\n",
    "3. **Calculate the 1-SE Threshold**: Add one standard error to the performance metric of the best model to set the 1-SE threshold.\n",
    "\n",
    "4. **Select the Simplest Model**: From the set of models, choose the simplest model whose performance is within the 1-SE threshold.\n",
    "\n",
    "This approach helps in avoiding overfitting by favoring simpler models that perform nearly as well as the more complex ones.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b93e0-9b69-44e7-a285-9e382fef5b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
